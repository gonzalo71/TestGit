{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597934617149",
   "display_name": "Python 3.7.7 64-bit ('tensorflow_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "You have TensorFlow version 2.1.0\n"
    }
   ],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "# print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "layers = keras.layers\n",
    "models = keras.models\n",
    "\n",
    "\n",
    "# This code was tested with TensorFlow v1.8\n",
    "print(\"You have TensorFlow version\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   category                                               text\n0      tech  tv future in the hands of viewers with home th...\n1  business  worldcom boss  left books alone  former worldc...\n2     sport  tigers wary of farrell  gamble  leicester say ...\n3     sport  yeading face newcastle in fa cup premiership s...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>category</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>tech</td>\n      <td>tv future in the hands of viewers with home th...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>business</td>\n      <td>worldcom boss  left books alone  former worldc...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sport</td>\n      <td>tigers wary of farrell  gamble  leicester say ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sport</td>\n      <td>yeading face newcastle in fa cup premiership s...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "url = r\"C:\\Users\\gonza\\OneDrive\\Documentos\\datascience-thebridge-master\\ejercicios gonzalo\\DATA\\bbc-text.csv\"\n",
    "data = pd.read_csv(url)\n",
    "data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "sport            0.229663\nbusiness         0.229213\npolitics         0.187416\ntech             0.180225\nentertainment    0.173483\nName: category, dtype: float64"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "data[\"category\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train size: 1780\nTest size: 445\n"
    }
   ],
   "source": [
    "train_size = int(len(data) * .8)\n",
    "print(f\"Train size: {train_size}\")\n",
    "print(f\"Test size: {len(data)-train_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, train_size):\n",
    "    train = data[:train_size]\n",
    "    test = data[train_size:]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "There's some work to be done in order for our data to be ready for training.\n",
    "\n",
    "#First we'll split the data into training and test sets.\n",
    "#Then we'll tokenize the words (text), and then convert them to a numbered index.\n",
    "#Next we'll do the same for the labels (categories), by using the LabelEncoder utility.\n",
    "#Finally, we'll convert the labels to a one-hot representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat, test_cat = train_test_split(data[\"category\"], train_size)\n",
    "train_text, test_text = train_test_split(data[\"text\"],train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "tokenize = keras.preprocessing.text.Tokenizer(num_words=max_words, char_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize.fit_on_texts(train_text) # fit tokenizer to our trian data\n",
    "x_train = tokenize.texts_to_matrix(train_text)\n",
    "x_test = tokenize.texts_to_matrix(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sklearn to convert label strings to numbered index (ojo solo usamos transform en test no fit)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_cat)\n",
    "y_train = encoder.transform(train_cat)\n",
    "y_test = encoder.transform(test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the labels to a one hot represantion\n",
    "num_classes = np.max(y_train) +1 \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "x_train shape: (1780, 1000)\nx_test shape: (445, 1000)\ny_train shape: (1780, 5)\ny_test shape: (445, 5)\n"
    }
   ],
   "source": [
    "# dimension of our training and test data\n",
    "print (\"x_train shape:\", x_train.shape)\n",
    "print (\"x_test shape:\", x_test.shape)\n",
    "print (\"y_train shape:\", y_train.shape)\n",
    "print (\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model trains very quickly and 2 epochs are already more than enough\n",
    "# Training for more epochs will likely lead to overfitting on this dataset\n",
    "# You can try tweaking these hyperparamaters when using this model with your own data\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "drop_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512,input_shape=(max_words,)))\n",
    "model.add(layers.Activation(\"relu\"))\n",
    "\n",
    "model.add(layers.Dense(num_classes))\n",
    "model.add(layers.Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=\"adam\",\n",
    "                metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 1602 samples, validate on 178 samples\nEpoch 1/2\n1602/1602 [==============================] - 2s 1ms/sample - loss: 0.4182 - accuracy: 0.8876 - val_loss: 0.1291 - val_accuracy: 0.9551\nEpoch 2/2\n1602/1602 [==============================] - 1s 357us/sample - loss: 0.0560 - accuracy: 0.9863 - val_loss: 0.1061 - val_accuracy: 0.9607\n"
    }
   ],
   "source": [
    "# model.fit trains the model\n",
    "# The validation_split param tells Keras what % of our training data should be used in the validation set\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "445/445 [==============================] - 0s 211us/sample - loss: 0.1415 - accuracy: 0.9551\ntest loss: 0.14149320239431404\ntest accuracy: 0.9550562\n"
    }
   ],
   "source": [
    "# Evaluate the accuracy of our trained model\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size,verbose=1)\n",
    "print(\"test loss:\", score[0])\n",
    "print(\"test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some predictions\n",
    "Take some samples from the test dataset and inspect some individual predictions, to ensure that things are sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "hobbit picture  four years away  lord of the rings ...\nActual label: entertainment\nPredicted label: entertainment\n\ngame firm holds  cast  auditions video game firm b ...\nActual label: tech\nPredicted label: sport\n\nclarke plans migrant point scheme anyone planning  ...\nActual label: politics\nPredicted label: politics\n\nradcliffe will compete in london paula radcliffe w ...\nActual label: sport\nPredicted label: sport\n\nserena becomes world number two serena williams ha ...\nActual label: sport\nPredicted label: sport\n\nultimate game  award for doom 3 sci-fi shooter doo ...\nActual label: tech\nPredicted label: tech\n\nalgeria hit by further gas riots algeria suffered  ...\nActual label: business\nPredicted label: business\n\nfast lifts rise into record books two high-speed l ...\nActual label: tech\nPredicted label: tech\n\nmuslim group attacks tv drama 24 a british muslim  ...\nActual label: entertainment\nPredicted label: entertainment\n\nus tv special for tsunami relief a us television n ...\nActual label: entertainment\nPredicted label: entertainment\n\n"
    }
   ],
   "source": [
    "# Here's how to generate a prediction on individual examples\n",
    "text_labels = encoder.classes_\n",
    "\n",
    "for i in range(10):\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction)]\n",
    "    print (test_text.iloc[i][:50], \"...\")\n",
    "    print (\"Actual label:\",  test_cat.iloc[i])\n",
    "    print (\"Predicted label:\",  predicted_label + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}