{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import random\n",
    "import os\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.ml  import Pipeline     \n",
    "from pyspark.sql import SQLContext  \n",
    "from pyspark.sql.functions import mean,col,split, col, regexp_extract, when, lit\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import QuantileDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/alextanhongpin/machine-learning-with-pyspark/master/07_recommender_system/movie_ratings_df.csv'\n",
    "csv = pd.read_csv(url, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   userId         title  rating\n",
       "0     196  Kolya (1996)       3\n",
       "1      63  Kolya (1996)       3\n",
       "2     226  Kolya (1996)       5"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId</th>\n      <th>title</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>196</td>\n      <td>Kolya (1996)</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>63</td>\n      <td>Kolya (1996)</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>226</td>\n      <td>Kolya (1996)</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "csv.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.to_csv(r\"C:\\Users\\gonza\\Repositorios\\TestGit\\movie_ratings_df.csv\",index=False)"
   ]
  },
  {
   "source": [
    "### Movie Recommendation with Pyspark "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sparksession\n",
    "spark = SparkSession.builder.appName('recomendation').master(\"local[5]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   userId         title  rating\n",
       "0     196  Kolya (1996)       3\n",
       "1      63  Kolya (1996)       3\n",
       "2     226  Kolya (1996)       5"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId</th>\n      <th>title</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>196</td>\n      <td>Kolya (1996)</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>63</td>\n      <td>Kolya (1996)</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>226</td>\n      <td>Kolya (1996)</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "url = r\"C:\\Users\\gonza\\Repositorios\\TestGit\\movie_ratings_df.csv\"\n",
    "\n",
    "df = spark.read.csv(url,inferSchema=True,header=True)\n",
    "df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- userId: integer (nullable = true)\n |-- title: string (nullable = true)\n |-- rating: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "source": [
    "As we can see, the title column is stored as string type. To work with pyspark Mlib library, we need to convert string type to numeric values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   userId         title  rating  title_new\n",
       "0     196  Kolya (1996)       3      287.0\n",
       "1      63  Kolya (1996)       3      287.0\n",
       "2     226  Kolya (1996)       5      287.0\n",
       "3     154  Kolya (1996)       3      287.0\n",
       "4     306  Kolya (1996)       5      287.0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId</th>\n      <th>title</th>\n      <th>rating</th>\n      <th>title_new</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>196</td>\n      <td>Kolya (1996)</td>\n      <td>3</td>\n      <td>287.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>63</td>\n      <td>Kolya (1996)</td>\n      <td>3</td>\n      <td>287.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>226</td>\n      <td>Kolya (1996)</td>\n      <td>5</td>\n      <td>287.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>154</td>\n      <td>Kolya (1996)</td>\n      <td>3</td>\n      <td>287.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>306</td>\n      <td>Kolya (1996)</td>\n      <td>5</td>\n      <td>287.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer,IndexToString\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol='title',outputCol='title_new')\n",
    "# Applying stringindexer object on dataframe movie title column\n",
    "model = stringIndexer.fit(df)\n",
    "#creating new dataframe with transformed values\n",
    "indexed = model.transform(df)\n",
    "indexed.limit(5).toPandas()"
   ]
  },
  {
   "source": [
    "We use Alternating least squares (ALS) algorithm in Pyspark Ml library for recommendation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   userId                          title  rating  title_new  prediction\n",
       "0      85  Much Ado About Nothing (1993)       4      148.0    3.674069\n",
       "1     588  Much Ado About Nothing (1993)       5      148.0    3.894752\n",
       "2     916  Much Ado About Nothing (1993)       4      148.0    3.853631\n",
       "3     253  Much Ado About Nothing (1993)       4      148.0    3.733635\n",
       "4     409  Much Ado About Nothing (1993)       3      148.0    3.455377"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId</th>\n      <th>title</th>\n      <th>rating</th>\n      <th>title_new</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>85</td>\n      <td>Much Ado About Nothing (1993)</td>\n      <td>4</td>\n      <td>148.0</td>\n      <td>3.674069</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>588</td>\n      <td>Much Ado About Nothing (1993)</td>\n      <td>5</td>\n      <td>148.0</td>\n      <td>3.894752</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>916</td>\n      <td>Much Ado About Nothing (1993)</td>\n      <td>4</td>\n      <td>148.0</td>\n      <td>3.853631</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>253</td>\n      <td>Much Ado About Nothing (1993)</td>\n      <td>4</td>\n      <td>148.0</td>\n      <td>3.733635</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>409</td>\n      <td>Much Ado About Nothing (1993)</td>\n      <td>3</td>\n      <td>148.0</td>\n      <td>3.455377</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# split the data\n",
    "train, test = indexed.randomSplit([0.75,0.25])\n",
    "\n",
    "# training the model using train dataset\n",
    "from pyspark.ml.recommendation import ALS\n",
    "rec = ALS(maxIter=10,\n",
    "            regParam=0.01,\n",
    "            userCol='userId',\n",
    "            itemCol='title_new',\n",
    "            ratingCol='rating',\n",
    "            nonnegative=True,\n",
    "            coldStartStrategy='drop')\n",
    "# fit the model on training set\n",
    "rec_model = rec.fit(train)\n",
    "# making prediction on test set\n",
    "predicted_ratings = rec_model.transform(test)\n",
    "predicted_ratings.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0221254544133624\n"
     ]
    }
   ],
   "source": [
    "# importing regression evaluator to measure rmse\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# creating regresion evaluator object for measuring accuracy\n",
    "evaluator = RegressionEvaluator(metricName='rmse',predictionCol='prediction',labelCol='rating')\n",
    "# apply the RE on prediction dataframe to calculate RMSE\n",
    "rmse = evaluator.evaluate(predicted_ratings)\n",
    "print(rmse)"
   ]
  },
  {
   "source": [
    "### After training, now is the time to recommend top movies which user might like"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create datase with all unique movies\n",
    "unique_movies = indexed.select('title_new').distinct()\n",
    "\n",
    "# create a function to recomended top \"n\" movies to any particular users\n",
    "def top_movies(user_id,n):\n",
    "    # assing alias name \"a\" to unique movies\n",
    "    a = unique_movies.alias('a')\n",
    "    # creating another dataframe which contains already watched movies by active user\n",
    "    watched_movies = indexed.filter(indexed['userId'] == user_id).select('title_new')\n",
    "    # assigning alias \n",
    "    b = watched_movies.alias('b')\n",
    "    # joining both tables on left join\n",
    "    total_movies = a.join(b,a.title_new==b.title_new,how='left')\n",
    "    # selecting movies which active user is yet to rate o watch\n",
    "    remaining_movies = total_movies.where(col('b.title_new').isNull()).select(a.title_new).distinct()\n",
    "    # adding new column of user_id\n",
    "    remaining_movies = remaining_movies.withColumn(\"userId\",lit(int(user_id)))\n",
    "    # making recomendation\n",
    "    recommendations = rec_model.transform(remaining_movies).orderBy(\"prediction\",ascending=False).limit(n)\n",
    "    # adding movie title\n",
    "    movie_title = IndexToString(inputCol=\"title_new\",outputCol=\"title\",labels=model.labels)\n",
    "    final_recomendation = movie_title.transform(recommendations)\n",
    "\n",
    "    return final_recomendation.show(n,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+------+----------+-----------------------------+\n|title_new|userId|prediction|title                        |\n+---------+------+----------+-----------------------------+\n|681.0    |60    |6.055451  |Kundun (1997)                |\n|1277.0   |60    |6.044452  |Mina Tannenbaum (1994)       |\n|993.0    |60    |6.0030537 |In the Bleak Midwinter (1995)|\n|1288.0   |60    |5.9719954 |Whole Wide World, The (1996) |\n|835.0    |60    |5.956031  |Vanya on 42nd Street (1994)  |\n+---------+------+----------+-----------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "top_movies(60,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    " spark.stop()"
   ]
  },
  {
   "source": [
    "### pyspark examples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "dataDF = [(('James','','Smith'),'1991-04-01','M',3000),\n",
    "  (('Michael','Rose',''),'2000-05-19','M',4000),\n",
    "  (('Robert','','Williams'),'1978-09-05','M',4000),\n",
    "  (('Maria','Anne','Jones'),'1967-12-01','F',4000),\n",
    "  (('Jen','Mary','Brown'),'1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('dob', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('gender', IntegerType(), True)\n",
    "         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- gender: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# parallelize\n",
    "df = spark.createDataFrame(data= dataDF,schema=schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- date_of_birth: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- gender: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# cambiar nombre\n",
    "df.withColumnRenamed(\"dob\",\"date_of_birth\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- newCol1: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- newCol2: string (nullable = true)\n |-- newCol3: string (nullable = true)\n |-- newCol4: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "new_col = [\"newCol1\",\"newCol2\",\"newCol3\",\"newCol4\"]\n",
    "df.toDF(*new_col).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- nombre: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- nacimiento: string (nullable = true)\n |-- genero: string (nullable = true)\n |-- salario: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# with column\n",
    "nuevas_col = [\"nombre\",\"nacimiento\",\"genero\",\"salario\"]\n",
    "df3 = df.toDF(*nuevas_col)\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+----+\n|genero|salario|pais|\n+------+-------+----+\n|M     |3000   |usa |\n|M     |4000   |usa |\n|M     |4000   |usa |\n|F     |4000   |usa |\n|F     |-1     |usa |\n+------+-------+----+\n\n"
     ]
    }
   ],
   "source": [
    "df4 = df3.select(col(\"genero\"),col(\"salario\"),lit('usa').alias(\"pais\"))\n",
    "df4.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+----+------------+\n|genero|salario|pais|tipo_salario|\n+------+-------+----+------------+\n|M     |3000   |usa |peque単o     |\n|M     |4000   |usa |grande      |\n|M     |4000   |usa |grande      |\n|F     |4000   |usa |grande      |\n|F     |-1     |usa |peque単o     |\n+------+-------+----+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df5 = df4.withColumn(\"tipo_salario\",when(col(\"salario\") >= 4000,lit(\"grande\")).otherwise(lit(\"peque単o\")))\n",
    "df5.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+----+------------+\n|genero|salario|pais|tipo_salario|\n+------+-------+----+------------+\n|M     |3000   |usa |peque単o     |\n|M     |4000   |usa |grande      |\n|M     |4000   |usa |grande      |\n+------+-------+----+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df5.filter(df5.genero == \"M\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "source": [
    "### order by and sort explained"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000), \\\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|James        |Sales     |NY   |90000 |34 |10000|\n+-------------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"department\",\"state\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n+-------------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"EMP\")\n",
    "spark.sql(\"select employee_name,department,state,salary,age,bonus from EMP ORDER BY department asc\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+--------+\n|department|media   |\n+----------+--------+\n|Finance   |87750.0 |\n|Sales     |85666.67|\n|Marketing |85500.0 |\n+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select department,round(avg(salary),2) as media from EMP GROUP BY 1 ORDER BY 2 DESC \"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}