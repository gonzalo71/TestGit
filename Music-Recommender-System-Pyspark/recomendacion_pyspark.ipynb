{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.mllib.recommendation import *\n",
    "import random\n",
    "from operator import *\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkContext(master=\"local[2]\",appName=\"recomendation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "artistData = spark.textFile(r\"C:\\Users\\gonza\\Repositorios\\TestGit\\Music-Recommender-System-Pyspark\\artist_data_small.txt\") \\\n",
    "    .map(lambda s:(int(s.split(\"\\t\")[0]),s.split(\"\\t\")[1]))   \n",
    "    \n",
    "artistAlias = spark.textFile(r\"C:\\Users\\gonza\\Repositorios\\TestGit\\Music-Recommender-System-Pyspark\\artist_alias_small.txt\")\n",
    "userArtistData = spark.textFile(r\"C:\\Users\\gonza\\Repositorios\\TestGit\\Music-Recommender-System-Pyspark\\user_artist_data_small.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the three users with the highest number of total play counts (sum of all counters) and print the user ID, the total play count, and the mean play count (average number of times a user played an artist)\n",
    "# split\n",
    "userArtistData = userArtistData.map(lambda s:(int(s.split(\" \")[0]),int(s.split(\" \")[1]),int(s.split(\" \")[2])))\n",
    "# create a dict\n",
    "artistAliasDict = {}\n",
    "dataValue = artistAlias.map(lambda s:(int(s.split(\"\\t\")[0]),int(s.split(\"\\t\")[1])))\n",
    "for temp in dataValue.collect():\n",
    "    artistAliasDict[temp[0]] = temp[1]\n",
    "\n",
    "# If artistid exists, replace with artistsid from artistAlias, else retain original\n",
    "userArtistData = userArtistData.map(lambda x:(x[0],artistAliasDict[x[1]] if x[1] in artistAliasDict else x[1],x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an RDD consisting of 'userid' and 'playcount' objects of original tuple\n",
    "userSum = userArtistData.map(lambda x:(x[0],x[2]))\n",
    "playCount1 = userSum.map(lambda x:(x[0],x[1])).reduceByKey(lambda a,b:a+b)\n",
    "playCount2 = userSum.map(lambda x:(x[0],1)).reduceByKey(lambda a,b:a+b)\n",
    "playSumAndCount = playCount1.leftOuterJoin(playCount2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "type(playCount2)"
   ]
  }
 ]
}