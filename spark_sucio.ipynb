{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,FloatType,LongType\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CAPTAIN AMERICAis the most popular hero1937\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"PopularHero\").getOrCreate()\n",
    "\n",
    "schema = StructType([\\\n",
    "                    StructField(\"id\",IntegerType(),True),\\\n",
    "                    StructField(\"name\",StringType(),True)\\\n",
    "    ])\n",
    "\n",
    "names = spark.read.schema(schema).option(\"sep\",\" \").csv(\"file:///SparkCourse/Marvel-names.txt\")\n",
    "\n",
    "lines = spark.read.text(\"file:///SparkCourse/Marvel-graph.txt\")\n",
    "\n",
    "# we trim each line of whitespace\n",
    "connections = lines.withColumn(\"id\",func.split(func.col(\"value\"), \" \")[0]) \\\n",
    "    .withColumn(\"connections\",func.size(func.split(func.col(\"value\"),\" \"))-1) \\\n",
    "        .groupby(\"id\").agg(func.sum(\"connections\").alias(\"connections\"))\n",
    "\n",
    "mostPopular = connections.sort(func.col(\"connections\").desc()).first()\n",
    "mostPopularName = names.filter(func.col(\"id\") == mostPopular[0]).select(\"name\").first()\n",
    "\n",
    "print(mostPopularName[0] + \"is the most popular hero\" + str(mostPopular[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                    .master(\"local[2]\")\\\n",
    "                    .appName(\"Examples\")\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"language\",\"users_count\"]\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
    "# creating dataframe from rdd\n",
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- _1: string (nullable = true)\n |-- _2: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "dfFromRDD1 = rdd.toDF()\n",
    "dfFromRDD1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- language: string (nullable = true)\n |-- user_count: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "columns = [\"language\",\"user_count\"]\n",
    "dfFromRDD1 = rdd.toDF(columns)\n",
    "dfFromRDD1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# otra forma\n",
    "dfFromRDD2 = spark.createDataFrame(rdd).toDF(*columns)\n",
    "dfFromRDD2.printSchema()\n",
    "\n",
    "# crear df vacio\n",
    "df_vacio = spark.createDataFrame(spark.sparkContext.emptyRDD(),schema) # schema seria la estructura:dtype,nombre columnas..\n",
    "df_vacio1 = spark.sparkContext.parallelize([]).toDF(schema)\n",
    "df_vacio2 = spark.createDataFrame([],schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- middle_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "|first_name|middle_name|last_name|  dob|gender|salary|\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "|     James|           |    Smith|36636|     M| 60000|\n",
      "|   Michael|       Rose|         |40288|     M| 70000|\n",
      "|    Robert|           | Williams|42114|      |400000|\n",
      "|     Maria|       Anne|    Jones|39192|     F|500000|\n",
      "|       Jen|       Mary|    Brown|     |     F|     0|\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pandas: se puede convertir un pyspark dataframe a panda, para operar con el con ml, pero despues habra que convertirlo de nuevo a \n",
    "# pyspark dataframe para que tengas todas las ventajas de procesamiento de spark ( con pandas es mas lento)\n",
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n",
    "        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n",
    "        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n",
    "        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n",
    "        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]\n",
    "columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n",
    "#df\n",
    "pysparkDF = spark.createDataFrame(data=data,schema=columns)\n",
    "pysparkDF.printSchema()\n",
    "pysparkDF.show(truncate=True)"
   ]
  },
  {
   "source": [
    "### Convert PySpark Dataframe to Pandas DataFrame\n",
    "PySpark DataFrame provides a method toPandas() to convert it Python Pandas DataFrame.\n",
    "\n",
    "toPandas() results in the collection of all records in the PySpark DataFrame to the driver program and should be done on a small subset of the data. running on larger datasetâ€™s results in memory error and crashes the application."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  first_name middle_name last_name    dob gender  salary\n0      James                 Smith  36636      M   60000\n1    Michael        Rose            40288      M   70000\n2     Robert              Williams  42114         400000\n3      Maria        Anne     Jones  39192      F  500000\n4        Jen        Mary     Brown             F       0\n"
     ]
    }
   ],
   "source": [
    "pandasDf = pysparkDF.toPandas()\n",
    "print(pandasDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.textFile(r\"C:\\SparkCourse\\book.txt\")\n",
    "# flatmap\n",
    "rdd2 = rdd.flatMap(lambda x:x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map\n",
    "rdd3 = rdd2.map(lambda x:(x,1))\n",
    "# reduce by key\n",
    "rdd4 = rdd3.reduceByKey(lambda a,b:(a+b))\n",
    "# sort\n",
    "rdd5 = rdd4.map(lambda x:(x[1],x[0])).sortByKey()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = spark.createDataFrame(data=rdd5,schema=[\"palabras\",\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+----------------+\n|palabras|           count|\n+--------+----------------+\n|       1|Self-Employment:|\n|       1|      Disclaimer|\n|       1|        Decision|\n|       1|          Career|\n|       1|             Ego|\n+--------+----------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "count 6993\n"
     ]
    }
   ],
   "source": [
    "print(\"count \" +str(final_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5c6b4d5d9db0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition and colaesence\n",
    "spark = SparkSession.builder.appName(\"ejemplos\").master(\"local[5]\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|James    |Smith   |USA    |CA   |\n",
      "|Michael  |Rose    |USA    |NY   |\n",
      "|Robert   |Williams|USA    |CA   |\n",
      "|Maria    |Jones   |USA    |FL   |\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "+---------+--------+-------+----------+\n",
      "|firstname|lastname|country|state     |\n",
      "+---------+--------+-------+----------+\n",
      "|James    |Smith   |USA    |California|\n",
      "|Michael  |Rose    |USA    |New York  |\n",
      "|Robert   |Williams|USA    |California|\n",
      "|Maria    |Jones   |USA    |Florida   |\n",
      "+---------+--------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# broadcast variables\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "(\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "(\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "(\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = df.rdd.map(lambda x:(x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
    "result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15\n",
      "15\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# acumulator\n",
    "accum = spark.sparkContext.accumulator(0)\n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd.foreach(lambda x:accum.add(x))\n",
    "print(accum.value)\n",
    "\n",
    "accuSum = spark.sparkContext.accumulator(0)\n",
    "def countFun(x):\n",
    "    global accuSum\n",
    "    accuSum+=x\n",
    "\n",
    "rdd.foreach(countFun)\n",
    "print(accuSum.value)\n",
    "\n",
    "accumCount = spark.sparkContext.accumulator(0)\n",
    "rdd2 = spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd2.foreach(lambda x:accumCount.add(x))\n",
    "print(accumCount.value)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "James,40\nAlice\n"
     ]
    }
   ],
   "source": [
    "row = Row(\"James\",40)\n",
    "print(row[0] + \",\" + str(row[1]))\n",
    "\n",
    "row2 = Row(name=\"Alice\",age=34)\n",
    "print(row2.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+--------+-------+-----+\n|firstname|lastname|country|state|\n+---------+--------+-------+-----+\n|James    |Smith   |USA    |CA   |\n|Michael  |Rose    |USA    |NY   |\n|Robert   |Williams|USA    |CA   |\n|Maria    |Jones   |USA    |FL   |\n+---------+--------+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# select\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+\n|firstname|\n+---------+\n|    James|\n|  Michael|\n|   Robert|\n|    Maria|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(\"firstname\").show() # igualmente puedo usar df.firstname o func.col(\"firstname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Row(firstname='James', lastname='Smith', country='USA', state='CA'), Row(firstname='Michael', lastname='Rose', country='USA', state='NY'), Row(firstname='Robert', lastname='Williams', country='USA', state='CA'), Row(firstname='Maria', lastname='Jones', country='USA', state='FL')]\n"
     ]
    }
   ],
   "source": [
    "# collect (recupera los datos, por eso mejor usarlo en small data y despues de hacer groupby, filter...)\n",
    "dataCollect = df.collect()\n",
    "print(dataCollect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "my name is  James  y vivo en  CA\nmy name is  Michael  y vivo en  NY\nmy name is  Robert  y vivo en  CA\nmy name is  Maria  y vivo en  FL\n"
     ]
    }
   ],
   "source": [
    "for row in dataCollect:\n",
    "    print(\"my name is \",row[\"firstname\"], \" y vivo en \",row[\"state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "import sys\n",
    "import codecs\n",
    "\n",
    "def loadMoviesNames():\n",
    "    movieNames = {}\n",
    "    with codecs.open(\"C:/SparkCourse/ml-100k/u.ITEM\",\"r\",encoding='ISO-8859-1',errors='ignore' ) as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Alsexample\").getOrCreate()\n",
    "\n",
    "moviesSchema = StructType([ \\\n",
    "                     StructField(\"userID\", IntegerType(), True), \\\n",
    "                     StructField(\"movieID\", IntegerType(), True), \\\n",
    "                     StructField(\"rating\", IntegerType(), True), \\\n",
    "                     StructField(\"timestamp\", LongType(), True)])\n",
    "\n",
    "names = loadMoviesNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training recomendation model ...\n"
     ]
    }
   ],
   "source": [
    "ratings = spark.read.option(\"sep\",\"\\t\").schema(moviesSchema)\\\n",
    "    .csv(\"C:/SparkCourse/ml-100k/u.data\")\n",
    "\n",
    "print(\"Training recomendation model ...\")\n",
    "\n",
    "als = ALS().setMaxIter(5).setRegParam(0.01).setUserCol(\"userID\")\\\n",
    "    .setItemCol(\"movieID\").setRatingCol(\"rating\")\n",
    "\n",
    "model = als.fit(ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Top 10 recommendations for user ID1\nSecret Agent, The (1996)7.2776384353637695\nAmerican Dream (1990)6.270341873168945\nTraveller (1997)6.146881580352783\nDangerous Beauty (1998)6.122129440307617\nAngel Baby (1995)6.0209856033325195\nPather Panchali (1955)6.001928329467773\nUnderneath, The (1995)5.852176666259766\nAparajito (1956)5.84736442565918\nFear of a Black Hat (1993)5.591094493865967\nFour Days in September (1997)5.564853191375732\n"
     ]
    }
   ],
   "source": [
    "# manually construct a dataframe of the user ID we wants rec for\n",
    "userID = 10\n",
    "userSchema = StructType([StructField(\"userID\",IntegerType(),True)])\n",
    "users = spark.createDataFrame([[userID,]],userSchema)\n",
    "\n",
    "recommendations = model.recommendForUserSubset(users, 10).collect()\n",
    "\n",
    "print(\"Top 10 recommendations for user ID\" + str(userID))\n",
    "\n",
    "for userRecs in recommendations:\n",
    "    myRecs = userRecs[1]\n",
    "    for rec in myRecs:\n",
    "        movie = rec[0]\n",
    "        rating = rec[1]\n",
    "        movieName = names[movie]\n",
    "        print(movieName + str(rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(-2.6468024342930363, -3.74)\n(-1.814069449462319, -2.58)\n(-1.658814147205745, -2.29)\n(-1.5459012001100543, -2.27)\n(-1.39064589785348, -2.09)\n(-1.4400453122078445, -2.07)\n(-1.4259311938208832, -2.0)\n(-1.369474720273038, -1.94)\n(-1.2918470691447508, -1.91)\n(-1.3130182467251927, -1.91)\n(-1.2212764772099443, -1.79)\n(-1.1930482404360216, -1.75)\n(-1.0307358789859664, -1.67)\n(-1.2142194180164636, -1.61)\n(-1.1648200036620988, -1.58)\n(-1.1859911812425408, -1.53)\n(-0.9742794054381211, -1.48)\n(-1.0307358789859664, -1.47)\n(-0.995450583018563, -1.46)\n(-0.995450583018563, -1.36)\n(-0.917822931890276, -1.34)\n(-0.8049099847945854, -1.3)\n(-0.833138221568508, -1.3)\n(-0.8401952807619887, -1.27)\n(-0.85430939914895, -1.26)\n(-0.8472523399554693, -1.25)\n(-0.7696246888271823, -1.24)\n(-0.8401952807619887, -1.23)\n(-0.8825376359228726, -1.16)\n(-0.8119670439880661, -1.14)\n(-0.7766817480206629, -1.12)\n(-0.8684235175359113, -1.11)\n(-0.6496546825380111, -1.1)\n(-0.7625676296337016, -1.1)\n(-0.833138221568508, -1.09)\n(-0.7907958664076242, -1.08)\n(-0.7061111560858563, -1.05)\n(-0.755510570440221, -1.03)\n(-0.7696246888271823, -0.99)\n(-0.7272823336662982, -0.98)\n(-0.6425976233445304, -0.97)\n(-0.7061111560858563, -0.94)\n(-0.7202252744728176, -0.94)\n(-0.7272823336662982, -0.91)\n(-0.7343393928597789, -0.89)\n(-0.6425976233445304, -0.85)\n(-0.5861411497966851, -0.84)\n(-0.6284835049575691, -0.84)\n(-0.49439938028143665, -0.82)\n(-0.6284835049575691, -0.81)\n(-0.5155705578618786, -0.8)\n(-0.5649699722162432, -0.8)\n(-0.5720270314097239, -0.78)\n(-0.4661711435075141, -0.76)\n(-0.5085134986683979, -0.75)\n(-0.6567117417314917, -0.74)\n(-0.6214264457640885, -0.71)\n(-0.52968467624884, -0.7)\n(-0.5367417354423206, -0.68)\n(-0.47322820270099475, -0.67)\n(-0.40265761076618817, -0.65)\n(-0.45911408431403344, -0.65)\n(-0.6214264457640885, -0.65)\n(-0.4802852618944754, -0.62)\n(-0.39560055157270757, -0.6)\n(-0.2756305452835364, -0.59)\n(-0.38148643318574627, -0.57)\n(-0.4520570251205528, -0.55)\n(-0.21917407173569114, -0.53)\n(-0.3532581964118236, -0.53)\n(-0.36031525560530425, -0.53)\n(-0.43794290673359143, -0.53)\n(-0.24034524931613313, -0.51)\n(-0.30385878205745903, -0.51)\n(-0.487342321087956, -0.51)\n(-0.24740230850961376, -0.5)\n(-0.39560055157270757, -0.5)\n(-0.26151642689657506, -0.49)\n(-0.4520570251205528, -0.46)\n(-0.2756305452835364, -0.43)\n(-0.31797290044442034, -0.42)\n(-0.36031525560530425, -0.41)\n(-0.19094583496176856, -0.39)\n(-0.26857348609005577, -0.39)\n(-0.28974466367049767, -0.36)\n(-0.33208701883138164, -0.35)\n(-0.17683171657480723, -0.34)\n(-0.2756305452835364, -0.33)\n(-0.1486034798008846, -0.32)\n(-0.2121170125422105, -0.31)\n(-0.21917407173569114, -0.3)\n(-0.2968017228639783, -0.3)\n(-0.20505995334872984, -0.28)\n(-0.2968017228639783, -0.28)\n(-0.11331818383348133, -0.27)\n(-0.16271759818784592, -0.26)\n(-0.19800289415524921, -0.25)\n(-0.16977465738132658, -0.24)\n(-0.18388877576828788, -0.24)\n(-0.16977465738132658, -0.23)\n(-0.20505995334872984, -0.22)\n(-0.2826876044770171, -0.22)\n(-0.20505995334872984, -0.19)\n(-0.09920406544652002, -0.18)\n(-0.15566053899436527, -0.16)\n(-0.014519355124752161, -0.14)\n(-0.07803288786607805, -0.11)\n(-0.07803288786607805, -0.1)\n(-0.09214700625303936, -0.1)\n(-0.11331818383348133, -0.09)\n(-0.12037524302696198, -0.09)\n(0.04193711842309308, -0.05)\n(0.01370888164917046, -0.05)\n(0.07722241439049636, -0.02)\n(0.027823000036131768, -0.02)\n(0.048994177616573736, 0.0)\n(-0.09920406544652002, 0.0)\n(0.04193711842309308, 0.01)\n(0.034880059229612424, 0.02)\n(0.027823000036131768, 0.03)\n(-0.007462295931271507, 0.04)\n(-0.06391876947911673, 0.07)\n(0.07016535519701571, 0.08)\n(0.04193711842309308, 0.08)\n(-0.028633473511713473, 0.08)\n(0.07722241439049636, 0.09)\n(-0.007462295931271507, 0.09)\n(0.11250771035789964, 0.11)\n(0.06310829600353504, 0.11)\n(0.048994177616573736, 0.13)\n(0.07722241439049636, 0.14)\n(0.24659183503403204, 0.15)\n(0.04193711842309308, 0.16)\n(0.26776301261447405, 0.17)\n(0.07016535519701571, 0.17)\n(0.06310829600353504, 0.17)\n(0.12662182874486091, 0.18)\n(0.18307830229270616, 0.2)\n(0.1760212430992255, 0.21)\n(0.1619071247122642, 0.21)\n(0.1619071247122642, 0.21)\n(0.12662182874486091, 0.21)\n(0.21836359826010943, 0.22)\n(0.21130653906662877, 0.23)\n(0.16896418390574486, 0.23)\n(0.12662182874486091, 0.25)\n(0.1971924206796675, 0.26)\n(0.14073594713182225, 0.26)\n(0.24659183503403204, 0.28)\n(0.1760212430992255, 0.29)\n(0.14779300632530287, 0.29)\n(0.3171624269688386, 0.3)\n(0.2536488942275127, 0.3)\n(0.20424947987314812, 0.3)\n(0.21130653906662877, 0.31)\n(0.28187713100143535, 0.34)\n(0.28187713100143535, 0.34)\n(0.2536488942275127, 0.37)\n(0.2959912493883966, 0.39)\n(0.20424947987314812, 0.39)\n(0.3171624269688386, 0.4)\n(0.2536488942275127, 0.4)\n(0.28893419019491595, 0.41)\n(0.2748200718079547, 0.41)\n(0.20424947987314812, 0.42)\n(0.2959912493883966, 0.43)\n(0.2959912493883966, 0.43)\n(0.2959912493883966, 0.45)\n(0.31010536777535797, 0.46)\n(0.3030483085818773, 0.46)\n(0.31010536777535797, 0.49)\n(0.26070595342099334, 0.49)\n(0.3383336045492806, 0.5)\n(0.4371324332580097, 0.51)\n(0.35950478212972253, 0.51)\n(0.39479007809712585, 0.53)\n(0.3312765453557999, 0.53)\n(0.5077030251928163, 0.54)\n(0.4794747884188937, 0.55)\n(0.4653606700319324, 0.58)\n(0.40184713729060645, 0.58)\n(0.4865318476123743, 0.59)\n(0.4230183148710484, 0.59)\n(0.3383336045492806, 0.61)\n(0.39479007809712585, 0.63)\n(0.4512465516449711, 0.68)\n(0.514760084386297, 0.69)\n(0.4441894924514904, 0.69)\n(0.6276730314819875, 0.7)\n(0.5712165579341423, 0.72)\n(0.5712165579341423, 0.72)\n(0.4653606700319324, 0.74)\n(0.4089041964840871, 0.74)\n(0.5641594987406616, 0.76)\n(0.4794747884188937, 0.77)\n(0.5077030251928163, 0.81)\n(0.6770724458363521, 0.82)\n(0.5712165579341423, 0.83)\n(0.514760084386297, 0.84)\n(0.4441894924514904, 0.89)\n(0.6559012682559101, 0.9)\n(0.6135589130950262, 0.9)\n(0.5853306763211035, 0.9)\n(0.5994447947080649, 0.93)\n(0.6206159722885068, 0.94)\n(0.7123577418037553, 0.95)\n(0.557102439547181, 0.95)\n(0.76175715615812, 0.96)\n(0.6911865642233134, 0.97)\n(0.6488442090624295, 0.97)\n(0.6347300906754682, 0.97)\n(0.6276730314819875, 0.97)\n(0.5994447947080649, 0.97)\n(0.6417871498689488, 0.98)\n(0.6417871498689488, 0.98)\n(0.6629583274493908, 0.99)\n(0.6841295050298327, 1.0)\n(0.6841295050298327, 1.01)\n(0.8111565705124845, 1.05)\n(0.5641594987406616, 1.05)\n(0.782928333738562, 1.06)\n(0.698243623416794, 1.1)\n(0.9099553992212137, 1.11)\n(0.860555984866849, 1.16)\n(0.8534989256733684, 1.19)\n(0.8111565705124845, 1.2)\n(0.9170124584146944, 1.22)\n(0.782928333738562, 1.27)\n(0.8958412808342524, 1.28)\n(1.0510965830908265, 1.32)\n(0.9946401095429814, 1.37)\n(1.044039523897346, 1.39)\n(0.9734689319625395, 1.39)\n(0.924069517608175, 1.42)\n(1.001697168736462, 1.44)\n(1.0510965830908265, 1.48)\n(1.001697168736462, 1.48)\n(1.065210701477788, 1.52)\n(1.0793248198647494, 1.54)\n(1.0722677606712687, 1.54)\n(1.0510965830908265, 1.54)\n(0.8676130440603297, 1.55)\n(1.0581536422843074, 1.63)\n(0.9946401095429814, 1.64)\n(1.1781236485734783, 1.66)\n(1.3757213059909368, 1.96)\n(1.5098054306670694, 2.09)\n(1.4251207203453014, 2.17)\n(1.6227183777627596, 2.31)\n(1.5168624898605498, 2.33)\n(1.6932889696975661, 2.39)\n(1.8344301535671794, 2.56)\n(-2.0398953436537, -2.89)\n(-1.8211265086558, -2.6)\n(-1.757612975914474, -2.54)\n(-1.7223276799470706, -2.45)\n(-1.7152706207535902, -2.36)\n(-1.4259311938208832, -2.0)\n(-1.305961187531712, -1.97)\n(-1.3412464834991153, -1.84)\n(-1.2636188323708282, -1.7)\n(-1.1295347076946958, -1.69)\n(-1.207162358822983, -1.68)\n(-1.1154205893077345, -1.63)\n(-1.0871923525338119, -1.6)\n(-1.1224776485012151, -1.59)\n(-1.0095647014055245, -1.57)\n(-0.9742794054381211, -1.5)\n(-0.9883935238250824, -1.5)\n(-0.9742794054381211, -1.43)\n(-1.0448499973729277, -1.42)\n(-0.917822931890276, -1.4)\n(-1.0307358789859664, -1.4)\n(-1.016621760599005, -1.39)\n(-0.9531082278576792, -1.37)\n(-0.8825376359228726, -1.36)\n(-0.7837388072141436, -1.3)\n(-0.7696246888271823, -1.23)\n(-0.9601652870511599, -1.2)\n(-0.6849399785054143, -1.16)\n(-0.7907958664076242, -1.16)\n(-0.6425976233445304, -1.14)\n(-0.833138221568508, -1.08)\n(-0.6355405641510498, -0.98)\n(-0.7343393928597789, -0.98)\n(-0.5579129130227626, -0.97)\n(-0.691997037698895, -0.96)\n(-0.6778829193119337, -0.95)\n(-0.7484535112467402, -0.93)\n(-0.7061111560858563, -0.92)\n(-0.7202252744728176, -0.92)\n(-0.6073123273771271, -0.91)\n(-0.5508558538292819, -0.9)\n(-0.6284835049575691, -0.9)\n(-0.6425976233445304, -0.89)\n(-0.5720270314097239, -0.88)\n(-0.5861411497966851, -0.85)\n(-0.7343393928597789, -0.82)\n(-0.5155705578618786, -0.81)\n(-0.6073123273771271, -0.81)\n(-0.670825860118453, -0.81)\n(-0.5790840906032044, -0.79)\n(-0.670825860118453, -0.79)\n(-0.670825860118453, -0.78)\n(-0.5508558538292819, -0.75)\n(-0.5649699722162432, -0.75)\n(-0.4802852618944754, -0.74)\n(-0.38148643318574627, -0.73)\n(-0.4167717291531495, -0.72)\n(-0.4802852618944754, -0.71)\n(-0.3885434923792269, -0.7)\n(-0.3885434923792269, -0.63)\n(-0.44499996592707214, -0.61)\n(-0.5226276170553592, -0.61)\n(-0.37442937399226556, -0.6)\n(-0.4238287883466301, -0.59)\n(-0.5437987946358013, -0.58)\n(-0.5508558538292819, -0.56)\n(-0.487342321087956, -0.55)\n(-0.2826876044770171, -0.54)\n(-0.39560055157270757, -0.52)\n(-0.33208701883138164, -0.47)\n(-0.31797290044442034, -0.44)\n(-0.3532581964118236, -0.44)\n(-0.2544593677030944, -0.42)\n(-0.34620113721834295, -0.42)\n(-0.22623113092917183, -0.41)\n(-0.2121170125422105, -0.4)\n(-0.15566053899436527, -0.39)\n(-0.31797290044442034, -0.39)\n(-0.12037524302696198, -0.38)\n(-0.3109158412509397, -0.37)\n(-0.17683171657480723, -0.35)\n(-0.2968017228639783, -0.35)\n(-0.22623113092917183, -0.34)\n(-0.22623113092917183, -0.34)\n(-0.2756305452835364, -0.34)\n(-0.17683171657480723, -0.33)\n(-0.18388877576828788, -0.31)\n(-0.11331818383348133, -0.3)\n(-0.2826876044770171, -0.3)\n(-0.19094583496176856, -0.28)\n(-0.38148643318574627, -0.26)\n(-0.2756305452835364, -0.25)\n(-0.04274759189867478, -0.24)\n(-0.09920406544652002, -0.21)\n(-0.22623113092917183, -0.21)\n(-0.23328819012265248, -0.21)\n(-0.23328819012265248, -0.2)\n(-0.1486034798008846, -0.19)\n(-0.0709758286725974, -0.18)\n(-0.21917407173569114, -0.18)\n(-0.04980465109215544, -0.17)\n(-0.12743230222044263, -0.15)\n(-0.028633473511713473, -0.13)\n(-0.10626112464000066, -0.11)\n(-0.0709758286725974, -0.1)\n(-0.007462295931271507, -0.09)\n(-0.014519355124752161, -0.07)\n(-0.04980465109215544, -0.07)\n(0.07722241439049636, -0.05)\n(-0.007462295931271507, -0.05)\n(-0.16271759818784592, -0.04)\n(-0.09920406544652002, -0.02)\n(-0.06391876947911673, -0.01)\n(0.07016535519701571, -0.0)\n(0.09839359197093833, -0.0)\n(-0.0004052367377908514, 0.02)\n(-0.014519355124752161, 0.03)\n(0.11250771035789964, 0.05)\n(0.05605123681005439, 0.08)\n(0.09839359197093833, 0.09)\n(0.09839359197093833, 0.11)\n(0.10545065116441897, 0.13)\n(0.048994177616573736, 0.13)\n(0.21836359826010943, 0.14)\n(0.19013536148618684, 0.15)\n(0.21836359826010943, 0.16)\n(0.2748200718079547, 0.18)\n(0.048994177616573736, 0.18)\n(0.06310829600353504, 0.19)\n(0.2254206574535901, 0.22)\n(0.21130653906662877, 0.24)\n(0.1971924206796675, 0.24)\n(0.15485006551878355, 0.24)\n(0.16896418390574486, 0.25)\n(0.20424947987314812, 0.26)\n(0.23247771664707076, 0.27)\n(0.19013536148618684, 0.29)\n(0.1619071247122642, 0.29)\n(0.18307830229270616, 0.3)\n(0.26776301261447405, 0.32)\n(0.14073594713182225, 0.32)\n(0.11956476955138029, 0.33)\n(0.19013536148618684, 0.34)\n(0.14073594713182225, 0.34)\n(0.3524477229362419, 0.35)\n(0.09133653277745767, 0.35)\n(0.3312765453557999, 0.37)\n(0.3665618413232032, 0.38)\n(0.32421948616231927, 0.38)\n(0.3877330189036452, 0.41)\n(0.39479007809712585, 0.45)\n(0.3665618413232032, 0.45)\n(0.21836359826010943, 0.45)\n(0.2395347758405514, 0.47)\n(0.4089041964840871, 0.48)\n(0.3171624269688386, 0.49)\n(0.21836359826010943, 0.51)\n(0.4089041964840871, 0.56)\n(0.35950478212972253, 0.58)\n(0.4865318476123743, 0.6)\n(0.39479007809712585, 0.61)\n(0.4512465516449711, 0.65)\n(0.4230183148710484, 0.65)\n(0.5218171435797776, 0.68)\n(0.4441894924514904, 0.68)\n(0.6135589130950262, 0.69)\n(0.5641594987406616, 0.73)\n(0.5712165579341423, 0.78)\n(0.4865318476123743, 0.78)\n(0.5712165579341423, 0.81)\n(0.76175715615812, 0.86)\n(0.6770724458363521, 0.86)\n(0.719414800997236, 0.87)\n(0.6770724458363521, 0.9)\n(0.6700153866428714, 0.91)\n(0.6559012682559101, 0.93)\n(0.6065018539015455, 0.95)\n(0.7405859785776779, 0.96)\n(0.5923877355145841, 0.97)\n(0.698243623416794, 0.98)\n(0.7335289193841973, 1.0)\n(0.6276730314819875, 1.04)\n(0.76175715615812, 1.05)\n(0.7123577418037553, 1.05)\n(0.782928333738562, 1.06)\n(0.782928333738562, 1.08)\n(0.782928333738562, 1.09)\n(0.6700153866428714, 1.1)\n(0.76175715615812, 1.13)\n(0.7476430377711586, 1.14)\n(0.8464418664798877, 1.2)\n(0.9381836359951363, 1.25)\n(0.924069517608175, 1.26)\n(0.9028983400277331, 1.31)\n(0.924069517608175, 1.34)\n(0.9522977543820976, 1.35)\n(1.0299254055103846, 1.36)\n(0.9099553992212137, 1.47)\n(1.1004959974451913, 1.5)\n(1.0722677606712687, 1.52)\n(1.022868346316904, 1.53)\n(1.2134089445408818, 1.54)\n(1.1710665893799976, 1.69)\n(1.1004959974451913, 1.74)\n(1.0722677606712687, 1.74)\n(1.248694240508285, 1.78)\n(1.2769224772822076, 1.85)\n(1.3192648324430916, 1.86)\n(1.2063518853474011, 1.93)\n(1.453348957119224, 1.98)\n(1.3051507140561303, 2.0)\n(1.5662619042149146, 2.08)\n(1.4674630755061853, 2.09)\n(1.636832496149721, 2.27)\n(2.0461419293715988, 2.71)\n(1.9755713374367923, 2.76)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #create spark session\n",
    "    spark = SparkSession.builder.appName(\"linear\").getOrCreate()\n",
    "    # load data and convert it to the formal mlib expects\n",
    "    inputLines = spark.sparkContext.textFile(\"C:/SparkCourse/regression.txt\")\n",
    "    data = inputLines.map(lambda x: x.split(\",\")).map(lambda x:(float(x[0]),Vectors.dense(float(x[1]))))\n",
    "    # convert this rdd to a dataframe\n",
    "    colNames = [\"label\",\"features\"]\n",
    "    df = data.toDF(colNames)\n",
    "    # there a lot of cases where you can avoid going from an Rdd to a dataframe(e.g you are importing from a real dataset o streaming)\n",
    "    # split data\n",
    "    trainTest = df.randomSplit([0.5,0.5])\n",
    "    trainingDF = trainTest[0]\n",
    "    testDf = trainTest[1]\n",
    "    # now create linear regresion model\n",
    "    lir = LinearRegression(maxIter=10,regParam=0.3,elasticNetParam=0.8)\n",
    "    # train model\n",
    "    model = lir.fit(trainingDF)\n",
    "    # now lets see if we can predict with out test data\n",
    "    fullPrediction = model.transform(testDf).cache()\n",
    "    predictions = fullPrediction.select(\"prediction\").rdd.map(lambda x:x[0])\n",
    "    labels = fullPrediction.select(\"label\").rdd.map(lambda x:x[0])\n",
    "    # zip together\n",
    "    predictionAndLabel = predictions.zip(labels).collect()\n",
    "    # print\n",
    "    for prediction in predictionAndLabel:\n",
    "        print(prediction)\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "source": [
    "### decision tree"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(40.0, 7.6)\n(16.36, 11.6)\n(24.69999999999999, 12.2)\n(16.36, 12.8)\n(18.9, 13.0)\n(14.233333333333334, 13.7)\n(14.033333333333331, 14.4)\n(14.033333333333331, 15.0)\n(16.36, 15.6)\n(14.033333333333331, 15.6)\n(16.36, 15.9)\n(16.36, 16.7)\n(14.033333333333331, 17.4)\n(24.69999999999999, 17.4)\n(23.75714285714286, 17.7)\n(16.36, 18.2)\n(14.233333333333334, 18.3)\n(16.36, 18.8)\n(24.69999999999999, 18.8)\n(14.233333333333334, 19.0)\n(14.233333333333334, 19.1)\n(18.9, 19.2)\n(16.36, 20.0)\n(16.36, 20.7)\n(23.75714285714286, 20.7)\n(32.56, 20.8)\n(27.772727272727273, 20.9)\n(27.772727272727273, 21.3)\n(27.772727272727273, 21.7)\n(27.772727272727273, 21.8)\n(35.69999999999999, 22.0)\n(27.772727272727273, 22.1)\n(14.033333333333331, 22.1)\n(32.56, 22.3)\n(14.033333333333331, 22.6)\n(27.772727272727273, 22.8)\n(27.772727272727273, 22.9)\n(40.0, 23.0)\n(27.772727272727273, 23.1)\n(14.233333333333334, 23.1)\n(27.772727272727273, 23.2)\n(27.772727272727273, 23.6)\n(27.772727272727273, 23.7)\n(14.233333333333334, 23.8)\n(27.772727272727273, 24.4)\n(32.56, 24.6)\n(27.772727272727273, 24.7)\n(35.69999999999999, 24.7)\n(27.772727272727273, 24.8)\n(40.0, 25.0)\n(32.56, 25.3)\n(14.033333333333331, 25.3)\n(32.56, 25.5)\n(27.772727272727273, 25.6)\n(23.75714285714286, 25.6)\n(35.69999999999999, 25.7)\n(23.75714285714286, 25.7)\n(32.56, 26.2)\n(40.0, 26.5)\n(32.56, 26.5)\n(32.56, 26.6)\n(44.40909090909091, 26.9)\n(27.772727272727273, 27.0)\n(23.75714285714286, 27.0)\n(27.772727272727273, 27.3)\n(16.36, 27.3)\n(27.772727272727273, 27.3)\n(27.772727272727273, 27.7)\n(27.772727272727273, 27.7)\n(23.75714285714286, 27.7)\n(27.772727272727273, 28.4)\n(32.56, 28.4)\n(34.550000000000004, 28.5)\n(27.772727272727273, 28.6)\n(40.0, 28.8)\n(32.56, 28.9)\n(35.69999999999999, 29.3)\n(32.56, 29.5)\n(27.772727272727273, 29.5)\n(40.0, 29.7)\n(34.75714285714286, 29.8)\n(34.75714285714286, 30.0)\n(32.56, 30.1)\n(27.772727272727273, 30.5)\n(40.0, 30.5)\n(23.75714285714286, 30.6)\n(27.772727272727273, 30.7)\n(40.0, 30.9)\n(27.772727272727273, 31.1)\n(27.772727272727273, 31.3)\n(40.0, 31.3)\n(40.02499999999999, 31.5)\n(27.772727272727273, 31.7)\n(40.02499999999999, 31.9)\n(27.772727272727273, 32.1)\n(32.56, 32.2)\n(40.0, 32.5)\n(40.0, 32.9)\n(40.02499999999999, 33.1)\n(32.56, 34.1)\n(40.0, 34.1)\n(40.02499999999999, 34.2)\n(32.56, 34.3)\n(40.0, 34.6)\n(40.0, 34.6)\n(34.75714285714286, 35.1)\n(34.75714285714286, 35.3)\n(40.0, 35.5)\n(34.75714285714286, 35.7)\n(44.40909090909091, 36.3)\n(40.0, 36.3)\n(40.0, 36.5)\n(40.0, 36.6)\n(32.56, 36.7)\n(40.0, 36.8)\n(40.02499999999999, 36.8)\n(44.40909090909091, 37.2)\n(34.550000000000004, 37.3)\n(40.0, 37.4)\n(40.0, 37.5)\n(40.0, 37.5)\n(32.56, 37.7)\n(40.0, 37.8)\n(32.56, 38.2)\n(34.75714285714286, 38.3)\n(48.58124999999999, 38.4)\n(34.75714285714286, 38.8)\n(40.0, 38.9)\n(40.0, 39.5)\n(44.40909090909091, 39.7)\n(40.02499999999999, 39.7)\n(40.0, 40.1)\n(40.0, 40.3)\n(48.58124999999999, 40.5)\n(40.0, 40.6)\n(48.58124999999999, 40.8)\n(40.0, 40.8)\n(44.40909090909091, 40.9)\n(40.0, 41.0)\n(52.51666666666667, 41.0)\n(40.0, 41.1)\n(40.0, 41.9)\n(40.0, 42.0)\n(44.40909090909091, 42.0)\n(44.40909090909091, 42.0)\n(43.23333333333333, 42.3)\n(40.0, 42.3)\n(40.0, 42.4)\n(40.0, 42.5)\n(40.0, 42.5)\n(44.40909090909091, 42.5)\n(48.58124999999999, 43.1)\n(65.68, 43.5)\n(40.0, 43.8)\n(46.26666666666667, 44.0)\n(48.5, 44.0)\n(44.40909090909091, 44.3)\n(52.51666666666667, 44.3)\n(48.58124999999999, 44.7)\n(48.58124999999999, 44.8)\n(65.68, 44.9)\n(48.58124999999999, 45.1)\n(48.58124999999999, 45.2)\n(65.68, 45.4)\n(46.26666666666667, 45.5)\n(40.0, 45.9)\n(40.0, 46.0)\n(44.40909090909091, 46.1)\n(44.40909090909091, 46.2)\n(40.0, 46.6)\n(44.40909090909091, 46.7)\n(39.4, 46.8)\n(48.58124999999999, 47.0)\n(44.40909090909091, 47.1)\n(56.370000000000005, 47.7)\n(40.0, 48.0)\n(52.51666666666667, 48.5)\n(48.58124999999999, 49.8)\n(34.550000000000004, 50.2)\n(40.02499999999999, 50.5)\n(48.58124999999999, 50.7)\n(44.40909090909091, 51.0)\n(40.0, 51.0)\n(44.40909090909091, 51.4)\n(44.40909090909091, 51.7)\n(48.58124999999999, 51.8)\n(46.26666666666667, 52.2)\n(65.68, 52.2)\n(48.58124999999999, 52.2)\n(56.370000000000005, 52.7)\n(44.40909090909091, 53.0)\n(44.40909090909091, 53.3)\n(48.58124999999999, 54.4)\n(48.58124999999999, 55.0)\n(44.40909090909091, 55.0)\n(44.40909090909091, 55.1)\n(46.26666666666667, 55.3)\n(40.0, 55.3)\n(39.4, 55.9)\n(48.58124999999999, 56.2)\n(56.370000000000005, 56.8)\n(48.58124999999999, 56.8)\n(48.58124999999999, 57.4)\n(56.370000000000005, 57.8)\n(56.370000000000005, 58.0)\n(56.370000000000005, 58.1)\n(56.370000000000005, 59.0)\n(56.370000000000005, 59.5)\n(44.40909090909091, 59.6)\n(48.58124999999999, 60.7)\n(40.02499999999999, 61.5)\n(40.02499999999999, 62.9)\n(48.58124999999999, 63.2)\n(65.68, 70.1)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create a SparkSession (Note, the config section is only for Windows!)\n",
    "    spark = SparkSession.builder.appName(\"DecisionTree\").getOrCreate()\n",
    "\n",
    "    \n",
    "    # Load up data as dataframe\n",
    "    data = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n",
    "        .csv('C:\\\\SparkCourse\\\\realestate.csv')\n",
    "\n",
    "    assembler = VectorAssembler().setInputCols([\"HouseAge\", \"DistanceToMRT\", \\\n",
    "                               \"NumberConvenienceStores\"]).setOutputCol(\"features\")\n",
    "    \n",
    "    df = assembler.transform(data).select(\"PriceOfUnitArea\", \"features\")\n",
    "\n",
    "    # Let's split our data into training data and testing data\n",
    "    trainTest = df.randomSplit([0.5, 0.5])\n",
    "    trainingDF = trainTest[0]\n",
    "    testDF = trainTest[1]\n",
    "\n",
    "    # Now create our decision tree\n",
    "    dtr = DecisionTreeRegressor().setFeaturesCol(\"features\").setLabelCol(\"PriceOfUnitArea\")\n",
    "\n",
    "    # Train the model using our training data\n",
    "    model = dtr.fit(trainingDF)\n",
    "\n",
    "    # Now see if we can predict values in our test data.\n",
    "    # Generate predictions using our decision tree model for all features in our\n",
    "    # test dataframe:\n",
    "    fullPredictions = model.transform(testDF).cache()\n",
    "\n",
    "    # Extract the predictions and the \"known\" correct labels.\n",
    "    predictions = fullPredictions.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "    labels = fullPredictions.select(\"PriceOfUnitArea\").rdd.map(lambda x: x[0])\n",
    "\n",
    "    # Zip them together\n",
    "    predictionAndLabel = predictions.zip(labels).collect()\n",
    "\n",
    "    # Print out the predicted and actual values for each point\n",
    "    for prediction in predictionAndLabel:\n",
    "      print(prediction)\n",
    "\n",
    "\n",
    "    # Stop the session\n",
    "    spark.stop()\n",
    "    \n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}