{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,FloatType,LongType\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CAPTAIN AMERICAis the most popular hero1937\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"PopularHero\").getOrCreate()\n",
    "\n",
    "schema = StructType([\\\n",
    "                    StructField(\"id\",IntegerType(),True),\\\n",
    "                    StructField(\"name\",StringType(),True)\\\n",
    "    ])\n",
    "\n",
    "names = spark.read.schema(schema).option(\"sep\",\" \").csv(\"file:///SparkCourse/Marvel-names.txt\")\n",
    "\n",
    "lines = spark.read.text(\"file:///SparkCourse/Marvel-graph.txt\")\n",
    "\n",
    "# we trim each line of whitespace\n",
    "connections = lines.withColumn(\"id\",func.split(func.col(\"value\"), \" \")[0]) \\\n",
    "    .withColumn(\"connections\",func.size(func.split(func.col(\"value\"),\" \"))-1) \\\n",
    "        .groupby(\"id\").agg(func.sum(\"connections\").alias(\"connections\"))\n",
    "\n",
    "mostPopular = connections.sort(func.col(\"connections\").desc()).first()\n",
    "mostPopularName = names.filter(func.col(\"id\") == mostPopular[0]).select(\"name\").first()\n",
    "\n",
    "print(mostPopularName[0] + \"is the most popular hero\" + str(mostPopular[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                    .master(\"local[2]\")\\\n",
    "                    .appName(\"Examples\")\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"language\",\"users_count\"]\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
    "# creating dataframe from rdd\n",
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- _1: string (nullable = true)\n |-- _2: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "dfFromRDD1 = rdd.toDF()\n",
    "dfFromRDD1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- language: string (nullable = true)\n |-- user_count: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "columns = [\"language\",\"user_count\"]\n",
    "dfFromRDD1 = rdd.toDF(columns)\n",
    "dfFromRDD1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# otra forma\n",
    "dfFromRDD2 = spark.createDataFrame(rdd).toDF(*columns)\n",
    "dfFromRDD2.printSchema()\n",
    "\n",
    "# crear df vacio\n",
    "df_vacio = spark.createDataFrame(spark.sparkContext.emptyRDD(),schema) # schema seria la estructura:dtype,nombre columnas..\n",
    "df_vacio1 = spark.sparkContext.parallelize([]).toDF(schema)\n",
    "df_vacio2 = spark.createDataFrame([],schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- middle_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "|first_name|middle_name|last_name|  dob|gender|salary|\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "|     James|           |    Smith|36636|     M| 60000|\n",
      "|   Michael|       Rose|         |40288|     M| 70000|\n",
      "|    Robert|           | Williams|42114|      |400000|\n",
      "|     Maria|       Anne|    Jones|39192|     F|500000|\n",
      "|       Jen|       Mary|    Brown|     |     F|     0|\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pandas: se puede convertir un pyspark dataframe a panda, para operar con el con ml, pero despues habra que convertirlo de nuevo a \n",
    "# pyspark dataframe para que tengas todas las ventajas de procesamiento de spark ( con pandas es mas lento)\n",
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n",
    "        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n",
    "        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n",
    "        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n",
    "        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]\n",
    "columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n",
    "#df\n",
    "pysparkDF = spark.createDataFrame(data=data,schema=columns)\n",
    "pysparkDF.printSchema()\n",
    "pysparkDF.show(truncate=True)"
   ]
  },
  {
   "source": [
    "### Convert PySpark Dataframe to Pandas DataFrame\n",
    "PySpark DataFrame provides a method toPandas() to convert it Python Pandas DataFrame.\n",
    "\n",
    "toPandas() results in the collection of all records in the PySpark DataFrame to the driver program and should be done on a small subset of the data. running on larger datasetâ€™s results in memory error and crashes the application."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  first_name middle_name last_name    dob gender  salary\n0      James                 Smith  36636      M   60000\n1    Michael        Rose            40288      M   70000\n2     Robert              Williams  42114         400000\n3      Maria        Anne     Jones  39192      F  500000\n4        Jen        Mary     Brown             F       0\n"
     ]
    }
   ],
   "source": [
    "pandasDf = pysparkDF.toPandas()\n",
    "print(pandasDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.textFile(r\"C:\\SparkCourse\\book.txt\")\n",
    "# flatmap\n",
    "rdd2 = rdd.flatMap(lambda x:x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map\n",
    "rdd3 = rdd2.map(lambda x:(x,1))\n",
    "# reduce by key\n",
    "rdd4 = rdd3.reduceByKey(lambda a,b:(a+b))\n",
    "# sort\n",
    "rdd5 = rdd4.map(lambda x:(x[1],x[0])).sortByKey()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = spark.createDataFrame(data=rdd5,schema=[\"palabras\",\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+----------------+\n|palabras|           count|\n+--------+----------------+\n|       1|Self-Employment:|\n|       1|      Disclaimer|\n|       1|        Decision|\n|       1|          Career|\n|       1|             Ego|\n+--------+----------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "count 6993\n"
     ]
    }
   ],
   "source": [
    "print(\"count \" +str(final_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5c6b4d5d9db0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition and colaesence\n",
    "spark = SparkSession.builder.appName(\"ejemplos\").master(\"local[5]\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|James    |Smith   |USA    |CA   |\n",
      "|Michael  |Rose    |USA    |NY   |\n",
      "|Robert   |Williams|USA    |CA   |\n",
      "|Maria    |Jones   |USA    |FL   |\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "+---------+--------+-------+----------+\n",
      "|firstname|lastname|country|state     |\n",
      "+---------+--------+-------+----------+\n",
      "|James    |Smith   |USA    |California|\n",
      "|Michael  |Rose    |USA    |New York  |\n",
      "|Robert   |Williams|USA    |California|\n",
      "|Maria    |Jones   |USA    |Florida   |\n",
      "+---------+--------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# broadcast variables\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "(\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "(\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "(\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = df.rdd.map(lambda x:(x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
    "result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15\n",
      "15\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# acumulator\n",
    "accum = spark.sparkContext.accumulator(0)\n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd.foreach(lambda x:accum.add(x))\n",
    "print(accum.value)\n",
    "\n",
    "accuSum = spark.sparkContext.accumulator(0)\n",
    "def countFun(x):\n",
    "    global accuSum\n",
    "    accuSum+=x\n",
    "\n",
    "rdd.foreach(countFun)\n",
    "print(accuSum.value)\n",
    "\n",
    "accumCount = spark.sparkContext.accumulator(0)\n",
    "rdd2 = spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd2.foreach(lambda x:accumCount.add(x))\n",
    "print(accumCount.value)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "James,40\nAlice\n"
     ]
    }
   ],
   "source": [
    "row = Row(\"James\",40)\n",
    "print(row[0] + \",\" + str(row[1]))\n",
    "\n",
    "row2 = Row(name=\"Alice\",age=34)\n",
    "print(row2.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+--------+-------+-----+\n|firstname|lastname|country|state|\n+---------+--------+-------+-----+\n|James    |Smith   |USA    |CA   |\n|Michael  |Rose    |USA    |NY   |\n|Robert   |Williams|USA    |CA   |\n|Maria    |Jones   |USA    |FL   |\n+---------+--------+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# select\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+\n|firstname|\n+---------+\n|    James|\n|  Michael|\n|   Robert|\n|    Maria|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(\"firstname\").show() # igualmente puedo usar df.firstname o func.col(\"firstname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Row(firstname='James', lastname='Smith', country='USA', state='CA'), Row(firstname='Michael', lastname='Rose', country='USA', state='NY'), Row(firstname='Robert', lastname='Williams', country='USA', state='CA'), Row(firstname='Maria', lastname='Jones', country='USA', state='FL')]\n"
     ]
    }
   ],
   "source": [
    "# collect (recupera los datos, por eso mejor usarlo en small data y despues de hacer groupby, filter...)\n",
    "dataCollect = df.collect()\n",
    "print(dataCollect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "my name is  James  y vivo en  CA\nmy name is  Michael  y vivo en  NY\nmy name is  Robert  y vivo en  CA\nmy name is  Maria  y vivo en  FL\n"
     ]
    }
   ],
   "source": [
    "for row in dataCollect:\n",
    "    print(\"my name is \",row[\"firstname\"], \" y vivo en \",row[\"state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "import sys\n",
    "import codecs\n",
    "\n",
    "def loadMoviesNames():\n",
    "    movieNames = {}\n",
    "    with codecs.open(\"C:/SparkCourse/ml-100k/u.ITEM\",\"r\",encoding='ISO-8859-1',errors='ignore' ) as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Alsexample\").getOrCreate()\n",
    "\n",
    "moviesSchema = StructType([ \\\n",
    "                     StructField(\"userID\", IntegerType(), True), \\\n",
    "                     StructField(\"movieID\", IntegerType(), True), \\\n",
    "                     StructField(\"rating\", IntegerType(), True), \\\n",
    "                     StructField(\"timestamp\", LongType(), True)])\n",
    "\n",
    "names = loadMoviesNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training recomendation model ...\n"
     ]
    }
   ],
   "source": [
    "ratings = spark.read.option(\"sep\",\"\\t\").schema(moviesSchema)\\\n",
    "    .csv(\"C:/SparkCourse/ml-100k/u.data\")\n",
    "\n",
    "print(\"Training recomendation model ...\")\n",
    "\n",
    "als = ALS().setMaxIter(5).setRegParam(0.01).setUserCol(\"userID\")\\\n",
    "    .setItemCol(\"movieID\").setRatingCol(\"rating\")\n",
    "\n",
    "model = als.fit(ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Top 10 recommendations for user ID1\nSecret Agent, The (1996)7.2776384353637695\nAmerican Dream (1990)6.270341873168945\nTraveller (1997)6.146881580352783\nDangerous Beauty (1998)6.122129440307617\nAngel Baby (1995)6.0209856033325195\nPather Panchali (1955)6.001928329467773\nUnderneath, The (1995)5.852176666259766\nAparajito (1956)5.84736442565918\nFear of a Black Hat (1993)5.591094493865967\nFour Days in September (1997)5.564853191375732\n"
     ]
    }
   ],
   "source": [
    "# manually construct a dataframe of the user ID we wants rec for\n",
    "userID = 10\n",
    "userSchema = StructType([StructField(\"userID\",IntegerType(),True)])\n",
    "users = spark.createDataFrame([[userID,]],userSchema)\n",
    "\n",
    "recommendations = model.recommendForUserSubset(users, 10).collect()\n",
    "\n",
    "print(\"Top 10 recommendations for user ID\" + str(userID))\n",
    "\n",
    "for userRecs in recommendations:\n",
    "    myRecs = userRecs[1]\n",
    "    for rec in myRecs:\n",
    "        movie = rec[0]\n",
    "        rating = rec[1]\n",
    "        movieName = names[movie]\n",
    "        print(movieName + str(rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}