{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit (conda)",
   "display_name": "Python 3.8.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "a063a9175d89378fd7561f6bc423ee307bddeb5f1748e456061b04aaac1df62f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Arboles de regresion\n",
    "* la diferencia es que el target puede ser una variable continua, numerica; por contra los otros arboles lo que hacen es clasificar\n",
    "* cuando hay una regresion no lineal muy complicada podemos siempre usar este"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "\n",
       "    black  lstat  medv  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>crim</th>\n      <th>zn</th>\n      <th>indus</th>\n      <th>chas</th>\n      <th>nox</th>\n      <th>rm</th>\n      <th>age</th>\n      <th>dis</th>\n      <th>rad</th>\n      <th>tax</th>\n      <th>ptratio</th>\n      <th>black</th>\n      <th>lstat</th>\n      <th>medv</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1</td>\n      <td>296</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2</td>\n      <td>242</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n      <td>21.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2</td>\n      <td>242</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n      <td>34.7</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# ejemplo de arboles de decision es el dataset de Boston para predecir precios casas\n",
    "url = r\"https://raw.githubusercontent.com/joanby/python-ml-course/master/datasets/boston/Boston.csv\"\n",
    "data = pd.read_csv(url)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# podemos hacerlo tambien son sklearn train test split\n",
    "col_names = data.columns.values.tolist()\n",
    "predictors = col_names[:13]\n",
    "target = col_names[13] # col names va de 0 a 13 ¡\n",
    "X = data[predictors]\n",
    "Y = data[target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "regtree = DecisionTreeRegressor(min_samples_split=10,min_samples_leaf=10,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(min_samples_leaf=10, min_samples_split=10, random_state=0)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "regtree.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = regtree.predict(data[predictors])\n",
    "data[\"preds\"] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       preds  medv\n",
       "0  22.840000  24.0\n",
       "1  22.840000  21.6\n",
       "2  37.190000  34.7\n",
       "3  33.753846  33.4\n",
       "4  33.753846  36.2"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>preds</th>\n      <th>medv</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>22.840000</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>22.840000</td>\n      <td>21.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>37.190000</td>\n      <td>34.7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>33.753846</td>\n      <td>33.4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>33.753846</td>\n      <td>36.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# ahora comparamos (vemos que mucha se repiten porque caen en la misma rama del arbol)\n",
    "data[[\"preds\",\"medv\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahora hacemos validacion cruzado para ver si el modelo es consistente\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=10,shuffle=True,random_state=1)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "score = cross_val_score(regtree,X,Y, scoring='neg_mean_squared_error',cv=cv,n_jobs=1) # aqui no usamos accuracy sino mean squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-13.56126026 -15.81815238 -16.80851914 -43.84847746  -9.83021945\n -17.25544008 -14.72465011 -32.41301362 -22.14347973  -9.53776745]\n-19.59409796803987\n"
     ]
    }
   ],
   "source": [
    "print(score)\n",
    "scor = np.mean(score)\n",
    "print(scor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esto nos dicen que las previsoines estan por encima o por debajo 19 puntos de las reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('crim', 0.033333095852660116)\n('zn', 0.013127929346484539)\n('indus', 0.0011307722576663543)\n('chas', 0.0)\n('nox', 0.022601410608427585)\n('rm', 0.6159856905321107)\n('age', 0.018480937834828907)\n('dis', 0.005432740754594164)\n('rad', 0.0)\n('tax', 0.0023089475509547024)\n('ptratio', 0.009677423859204467)\n('black', 0.0)\n('lstat', 0.27792105140306866)\n"
     ]
    }
   ],
   "source": [
    "# vemos la importancia de las features (vemos truco imprimir horzional * y sep=\\n)\n",
    "print(*list(zip(predictors,regtree.feature_importances_)),sep=\"\\n\")"
   ]
  },
  {
   "source": [
    "### Random forest (podemos usarlo tambien para regresion o clasificacion)\n",
    "* es como un cajon desastre en el mundo de los modelos\n",
    "* no necesita validacion cruzada ya que utiliza el sistema de bagging\n",
    "* se toman n observaciones para cada datset(sample) con reemplazo.\n",
    "* estos datasets se llaman muestras de bootstrap o bolsas\n",
    "* construimos un arbol de regresion para cada muestras con su correspondinte sample de variable predictora\n",
    "* la predicion final es el promedio de las observaciones de todos los arboles y para clasificacion, la clase con mayoria de votos en el conjunto de los arboles\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a hacerlo para el dataset de antes (primero una regresion)\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=10, n_jobs=2, oob_score=True)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# n_jobs = numero de tareas en paralelo que se ejecuta al mismo tiempo\n",
    "# oob score = true el modelo hace un muestro aleatorio\n",
    "# n_estimators = numero de arboles que queramos\n",
    "\n",
    "forest = RandomForestRegressor(n_jobs=2,oob_score=True,n_estimators=10)\n",
    "forest.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     rforest_pred  medv\n",
       "0       28.025000  24.0\n",
       "1       22.825000  21.6\n",
       "2       34.983333  34.7\n",
       "3       36.780000  33.4\n",
       "4       37.300000  36.2\n",
       "..            ...   ...\n",
       "501     28.700000  22.4\n",
       "502     22.750000  20.6\n",
       "503     26.800000  23.9\n",
       "504     26.750000  22.0\n",
       "505     21.433333  11.9\n",
       "\n",
       "[506 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rforest_pred</th>\n      <th>medv</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>28.025000</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>22.825000</td>\n      <td>21.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>34.983333</td>\n      <td>34.7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>36.780000</td>\n      <td>33.4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>37.300000</td>\n      <td>36.2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>28.700000</td>\n      <td>22.4</td>\n    </tr>\n    <tr>\n      <th>502</th>\n      <td>22.750000</td>\n      <td>20.6</td>\n    </tr>\n    <tr>\n      <th>503</th>\n      <td>26.800000</td>\n      <td>23.9</td>\n    </tr>\n    <tr>\n      <th>504</th>\n      <td>26.750000</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>505</th>\n      <td>21.433333</td>\n      <td>11.9</td>\n    </tr>\n  </tbody>\n</table>\n<p>506 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# podemos ver las predicciones \n",
    "data[\"rforest_pred\"] = forest.oob_prediction_\n",
    "data[[\"rforest_pred\",\"medv\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "22.397944043081665"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "# vemos el error cuadratico medio, aqui no hace falta cross validation\n",
    "sum((data[\"medv\"]-data[\"rforest_pred\"])**2) / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "22.397944043081665"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "np.mean((data[\"medv\"]-data[\"rforest_pred\"])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7346829921535208"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "forest.oob_score_ # similar al coef de determinacion r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7346829921535208"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(Y,forest.oob_prediction_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## podemos hacer tambien random forest clasification (por ejemplo para iris)\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X,Y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "max = MinMaxScaler()\n",
    "X = max.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_jobs=2, oob_score=True)"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clas = RandomForestClassifier(n_jobs=2,oob_score=True,n_estimators=100)\n",
    "rf_clas.fit(df_iris,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9466666666666667"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "rf_clas.oob_score_  "
   ]
  },
  {
   "source": [
    "### consideracions\n",
    "* suelen funcionar bastante bien\n",
    "* no hay que podar los arboeles, por lo que podemos obtener buenso resultados con bosques grandes\n",
    "* parametros:\n",
    "    * tamaño del nodo(min_samples_leaf) podemos jugar con este parametro (de serie = 1)\n",
    "    * numero arboels (n_estimators) aqui podemso de media 500 (no hace falta podarlos)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Al momento de ajustar el modelo, debemos tener en cuenta los siguientes hiperparámetros. Estos nos ayudarán a que el bosque de mejores resultados para cada ejercicio. Recuerda que esto no se trata de “copiar y pegar”!\n",
    "\n",
    "* n_estimators: será la cantidad de árboles que generaremos.\n",
    "* max_features: la manera de seleccionar la cantidad máxima de features para cada árbol.\n",
    "* min_sample_leaf: número mínimo de elementos en las hojas para permitir un nuevo split (división) del nodo.\n",
    "* oob_score: es un método que emula el cross-validation en árboles y permite mejorar la precisión y evitar overfitting.\n",
    "* boostrap: para utilizar diversos tamaños de muestras para entrenar. Si se pone en falso, * utilizará siempre el dataset completo.\n",
    "* n_jobs: si tienes multiples cores en tu CPU, puedes indicar cuantos puede usar el modelo al entrenar para acelerar el entrenamiento."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "https://mlcourse.ai/articles/topic5-part2-rf/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}